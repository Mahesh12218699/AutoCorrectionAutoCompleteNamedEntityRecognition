{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy pyspellchecker\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "gb3fuANCx_SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "spell.word_frequency.load_words([\"customWord1\", \"customWord2\"])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "D_6FqI5AiN2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "40lqE7a32SSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load pre-trained models\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "spell = SpellChecker()\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Function for Autocomplete\n",
        "def autocomplete(text, max_length=10):\n",
        "    \"\"\"Predict the next words based on input text using GPT-2.\"\"\"\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1, do_sample=True)\n",
        "\n",
        "    auto_complete_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return auto_complete_text[len(text):].strip()\n",
        "\n",
        "# Function for Spellchecking\n",
        "def spell_check(word):\n",
        "    \"\"\"Return the most likely corrected word and suggestions.\"\"\"\n",
        "    corrected_word = spell.correction(word)\n",
        "    suggestions = spell.candidates(word)\n",
        "    return corrected_word, suggestions if suggestions else set()\n",
        "\n",
        "# Function to evaluate prediction performance for Autocomplete and Spellchecking\n",
        "def evaluate_predictions(true_words, predicted_words):\n",
        "    \"\"\"Calculate Precision, Recall, and F1 Score.\"\"\"\n",
        "    # Ensure both lists have the same length by padding with empty strings\n",
        "    true_words = true_words[:len(predicted_words)]\n",
        "    precision = precision_score(true_words, predicted_words, average='micro')\n",
        "    recall = recall_score(true_words, predicted_words, average='micro')\n",
        "    f1 = f1_score(true_words, predicted_words, average='micro')\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER) per word\n",
        "def named_entity_recognition_per_word(text):\n",
        "    \"\"\"Identify named entities for each word in the text using SpaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    word_entities = {}\n",
        "    for token in doc:\n",
        "        if token.ent_type_ != \"\":  # Check if the word has an entity label\n",
        "            word_entities[token.text] = token.ent_type_\n",
        "        else:\n",
        "            word_entities[token.text] = \"None\"\n",
        "    return word_entities\n",
        "\n",
        "# NLP Pipeline\n",
        "def nlp_pipeline(user_input, true_words):\n",
        "    \"\"\"Perform Autocomplete (with GPT-2), Spellchecking, and NER.\"\"\"\n",
        "    words = user_input.split()\n",
        "\n",
        "    # Autocomplete suggestions\n",
        "    gpt2_predictions = autocomplete(user_input)\n",
        "    formatted_suggestions = [word.strip() for word in gpt2_predictions.split() if word]\n",
        "\n",
        "    # Spellcheck results\n",
        "    spellcheck_results = []\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        corrected_word, spell_suggestions = spell_check(word)\n",
        "        corrected_words.append(corrected_word)\n",
        "        spellcheck_results.append((word, corrected_word, list(spell_suggestions)))\n",
        "\n",
        "    # NER results per word\n",
        "    ner_results = named_entity_recognition_per_word(user_input)\n",
        "\n",
        "    # Evaluate predictions for spellchecking\n",
        "    precision, recall, f1 = evaluate_predictions(true_words, corrected_words)\n",
        "\n",
        "    return {\n",
        "        \"autocomplete_suggestions\": formatted_suggestions,\n",
        "        \"spellcheck_results\": spellcheck_results,\n",
        "        \"ner_results\": ner_results,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_input = input(\"Enter a word or sentence (or type 'exit' to quit): \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Exiting the program.\")\n",
        "            break  # This will break the while loop and exit the program\n",
        "\n",
        "        true_words = user_input.split()\n",
        "\n",
        "        results = nlp_pipeline(user_input, true_words)\n",
        "\n",
        "        print(\"\\nResults:\")\n",
        "        print(\"1. Autocomplete Suggestions:\", results['autocomplete_suggestions'])\n",
        "\n",
        "        print(\"2. Spellcheck results:\")\n",
        "        for original_word, corrected_word, suggestions in results['spellcheck_results']:\n",
        "            print(f\"   - {original_word}: corrected to '{corrected_word}', suggestions: {suggestions}\")\n",
        "\n",
        "        print(\"3. Named Entity Recognition (NER) Results:\")\n",
        "        if results['ner_results']:\n",
        "            for word, entity in results['ner_results'].items():\n",
        "                print(f\"   - {word}: {entity}\")\n",
        "        else:\n",
        "            print(\"   - No named entities found.\")\n",
        "\n",
        "        print(\"4. Evaluation Metrics for Spellchecking:\")\n",
        "        print(f\"   - Precision: {results['precision']:.4f}\")\n",
        "        print(f\"   - Recall: {results['recall']:.4f}\")\n",
        "        print(f\"   - F1 Score: {results['f1_score']:.4f}\")\n",
        "        print(\"\\n\") instead of using gpt2 for text suggestion can you generate some random dataset of 1000 features only for auto complete"
      ],
      "metadata": {
        "id": "4yxnrrFk2Wot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}